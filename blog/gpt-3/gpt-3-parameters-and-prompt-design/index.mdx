---
title: "gpt-3-parameters-and-prompt-design"
date: "2023-03-06"
tags: ["gpt-3"]
---



## 解释一下常用参数

例子：

```bash
curl https://api.openai.com/v1/completions \
 -H "Content-Type: application/json" \
 -H "Authorization: Bearer $OPENAI_API_KEY" \
 -d ‘{
 "model": "text-davinci-002",
 "prompt": "What are 5 key points I should know when studying Ancient Rome?",
 "temperature": 0.3,
 "max_tokens": 150,
 "top_p": 1,
 "frequency_penalty": 0,
 "presence_penalty": 0
}'
```



openai.Completion 提供 4中 model:

| 名称 | 描述 | 最大长度(token) | 训练数据 |
| --- | --- | --- | --- |
| text-davinci-003 | 最强大的 GPT-3 模型。能完成其他模型的所有任务，并且输出的长度更长、质量更高，并且更加遵循模板中指令。 | 4000 | 2021.06 |
| text-curie-001 | 也很强大。相比 Davinci 速度更快、成本更低。 | 2,048	| 2019.10 |
| text-babbage-001 | 擅长处理直接简单的任务，速度非常快并且成本很低。 | 2,048 | 2019.10 |
| text-ada-001 | 擅长处理非常简单的任务，速度最快、成本最低 | 2,048 | 2019.10 |

其中，在质量和速度两个维度权衡下，分为四个模型：

1. **Davinci**：<mark>最强模型</mark>, Davinci 擅长于处理需要对文本内容有**大量理解的任务**，例如针对特定受众的摘要和创意内容生成。Davinci 的另一个亮点是能够**理解文本的意图**，尤其擅长于解决**逻辑问题**和解释**人物的动机**，目前 Davinci 甚至能够解决一些涉及因果关系的困难问题。

    *擅长于：复杂的意图、因果，以及为特定受众进行摘要。*

2. **Curie**: 同样很强大，并且速度非常快。虽然在分析复杂文本方面不如 Davinci，但同样能够胜任许多任务，例如情感分类和摘要。Curie 还非常擅长回答问题和执行对话。

    *擅长于：翻译、高难度分类、情感分析和摘要。*

3. **Babbage**：可以执行相对简单的任务，例如较为直接的分类。 在语义搜索方面，它也非常有能力对文档与搜索查询的匹配程度进行排名。

    *擅长于：中等难度分类和语义搜索分类。*

4. **Ada**：最快的 GPT-3 模型，可以执行文本解析、地址更正和难度较低的分类任务。Ada 的性能通常可以通过提供更多上下文来提高。

    *擅长于：文本解析、简单分类、地址修正和关键词抽取*


通常情况下，使用最强的 `Davinci`。


### `prompt`

ChatGPT 主要的功能就是 <mark>文本续写</mark>，将用户给定的文本片段补充维完整的文档。

而 prompt 就是这个功能的入口，通常指的是一个输入的文本段落或短语，作为生成模型输出的起点或引导。

prompt 可以是一个问题、一段文字描述、一段对话或任何形式的文本输入，模型会基于prompt 所提供的上下文和语义信息，生成相应的输出文本。

<mark>设置 prompt 就是设置你与ChatGPT的对话模式。</mark>



### 其他参数

- `max_tokens`：回复的最大长度。注意，大部分的语言模型都只能处理一定长度内的文本，例如 2048 或 4096 个字符 (`token`)，因此我们输入的模板长度加上 `max_tokens` 不能超过这个限制。

    *通常情况下，一个 token 大概是 4 个字符左右*

- `temperature`：采样温度，值介于 0 到 2 之间。值越高，输出就越随机。例如你希望对于同一个问题，模型每次的回复都能有较大的差异，就可以设置为一个较高的值。该参数与 `top_p` 通常只建议修改其中一个。

    *采样温度在 0 到 1 之间，我们可以控制模型预测的随机性和创造性。*

- `top_p`：一种替代温度采样的方法，称为核采样。模型只考虑具有 `top_p` 概率质量的 `token` 的结果。例如设为 0.1 模型就只考虑构成前 10% 概率质量的 `token`。该参数与 `temperature` 通常只建议修改其中一个。
- `presence_penalty`：值介于 -2.0 到 2.0 之间。正值会根据是否已经出现在文本中来惩罚新生成的 `token`，从而鼓励模型生成新的内容，避免出现大段重复的文本。
- `frequency_penalty`：值介于 -2.0 到 2.0 之间。正值会根据新生成 `token` 在文本中的频率对其进行惩罚，从而降低模型逐字重复同一行的可能性。


## 尝试在不同语境下使用

> 不同语境即配置不同 prompt，抛砖英语


### 1. 语法纠正，”ChatGPT 教你学英语”系列

对于在语法上苦苦挣扎的作家和学生来说，技术一直是福音。过去，我们有拼写检查器和语法检查器来帮助我们发现错误。但是现在，即使是 GPT-3 也可以完成这项工作并捕获错误。

这对于学习外语或语法有困难的人尤其有帮助。

**参数**

```text
model: text-davinci-003
max-tokens: 60
temperature: 0
top-p: 1.0
frequency-penalty: 0.0
presence-penalty: 0.0
```

**prompt**：
```text
Correct this to standard English: I’m looking forward to see you
```

**response**:
```text
I’m looking forward to seeing you.
```

### 2. 摘要，提取文本关键信息

```text
model: text-davinci-003
max-tokens: 512
temperature: 0.7
top-p: 1.0
frequency-penalty: 0.0
presence-penalty: 0.0
```

**prompt**：
```text
对于下面的文本，为小学生生成一个活泼有趣的摘要:\n\n木星是距离太阳第五近的行星，也是太阳系中体积最大的行星，目前已知有92颗卫星。
天文学家很早就发现了这颗行星，罗马人以他们的神称这颗行星为朱庇特。古代中国则称木星为岁星、太岁，取其绕行天球一周约为12年，与地支相同之故，且产生了岁星纪年法。
到西汉时期，《史记‧天官书》作者天文学家司马迁从实际观测发现岁星呈青色，与“五行”学说联系在一起，正式把它命名为木星。
木星是颗巨行星，质量是太阳的千分之一，但却是太阳系其他行星质量总和的2.5倍。太阳系的行星中，木星和土星是气体巨星（天王星和海王星是冰巨星）。
```

**response**:
```text
小学生摘要：木星是太阳系中最大的行星，它距离太阳第五近，目前已知有92颗卫星。
古代人称它为朱庇特或岁星，它每12年绕行一次天球，古代中国把它叫做太岁，它的质量是太阳的千分之一，但却是太阳系其他行星质量总和的2.5倍。
它是一颗气体巨星（天王星和海王星是冰巨星）。
```

### 3. 更多

- 担任面试官
- 旅游指南
- ...

根据不同的 prompt， gpt也会产出相类似的答案，网上也有很多总结 prompt 的网站，比如：

- [ChatGPT 中文调教指南](https://chatguide.plexpt.com/)
- [unlocking-the-power-of-llms](https://github.com/howl-anderson/unlocking-the-power-of-llms)

gpt 的世界刚刚打开一条门缝，之后的世界会更丰满！

## 参考

- [ChatGPT 使用教程](https://xiaosheng.run/2023/02/18/how-to-use-chatgpt.html)
- [GPT 提示语](https://juejin.cn/post/7204115174412304443)
- [参数及提示设计](https://towardsdatascience.com/gpt-3-parameters-and-prompt-design-1a595dc5b405)
